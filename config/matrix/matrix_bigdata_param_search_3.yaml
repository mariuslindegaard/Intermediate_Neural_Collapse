---
# Neural collapse config file. All must be specified
Model:
  model-name: None            # Name of model in "our_models"
  embedding-layers: []        # Intermediate layers to use for NC loss with weightings. "^" used to indicate start of network.

Data:
  dataset-id: None       # Which dataset-getter to use. Note that shapes are provided by the dataset
  batch-size: 128        # Mini-batch size
  do-augmentation: False # Whether to do data augmentation

Optimizer:
  loss: mseloss
  weight-decay: None     # Weight decay
  lr: None               # Learning rate
  lr-decay: 0.1
  lr-decay-steps: 3      # Number of learning rate decay steps
  momentum: 0.9          # Optimizer momentum
  epochs: 600            # Number of epochs to train for
  warmup_epochs: 2       # Number of epochs to do linear lr-warmup for

Logging:
  # When to store weights and calculate measurements
  save-dir: logs/matrix/bigdata_param_search_3
  # log-interval: 10        # At what interval to log checkpoints. Always includes first 10 epochs
  log-epochs: [0, 1, 2, 3, 5, 10, 20, 30, 40, 50, 60, 80, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475, 500, 525, 550, 575, 600]  # Overrides log-interval

Measurements:
  measures: [Accuracy]

# Matrix for parsing into multiple configs to run with slurm
# Parsed as follows:
## All innermost values (leaves)  must be given in a list.
## For the current dictionairy level:
##   Any key other than "_Exclusive" is parsed and the innermost list of that key is used in the matrix.
##   If the key is _Exclusive, assume mutual exclusivity within the list of its contents,
##   and choose one of them to parse as another equivalent dictionary of values.
Matrix:
  _Exclusive:
    mlp_xlarge:
      Model:
        model-name: [mlp_xlarge]
        # embedding-layers: [[fc]]
      # Measurements:
      #   measures: [True]
      Optimizer:
        lr: [2, 1, 0.5, 0.2, 0.1]
    mlp_huge:
      Model:
        model-name: [mlp_huge]
        # embedding-layers: [[fc]]
      # Measurements:
      #   measures: [True]
      Optimizer:
        lr: [2, 1, 0.5, 0.2, 0.1, 0.05, 0.02]
    convnet_deep:
      Model:
        model-name: [convnet_deep]
        # embedding-layers: [[conv, fc]]
      # Measurements:
      #   measures: [Fast_StableRank]
      Optimizer:
        lr: [20, 10, 5, 2, 1, 0.5, 0.2]
    convnet_deepwide:
      Model:
        model-name: [convnet_deepwide]
        # embedding-layers: [[conv, fc]]
      # Measurements:
      #   measures: [Fast_StableRank]
      Data:
        batch-size: [32]
      Optimizer:
        lr: [5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02]
    resnet152:
      Model:
        model-name: [resnet152]
        # embedding-layers: [[
        #   .conv1, .conv2, .conv3,
        #   ^avgpool, ^fc
        # ]]
      # Measurements:
      #   measures: [Fast_StableRank]
      Data:
        batch-size: [32]
      Optimizer:
        lr: [2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]
    resnet152_pretrained:
      Model:
        model-name: [resnet152_pretrained]
        # embedding-layers: [[
        #   .conv1, .conv2, .conv3,
        #   ^avgpool, ^fc
        # ]]
      # Measurements:
      #   measures: [Fast_StableRank]
      Data:
        batch-size: [32]
      Optimizer:
        lr: [3, 1, 0.3, 0.1, 0.03, 0.01]
  Data:
    dataset-id: [tinyimagenet]  # , stl10, cifar100]
    # dataset-id: [tinyimagenet, cifar100]  # , stl10, cifar100]
    # dataset-id: [MNIST, FashionMNIST, cifar10, svhn]  # , stl10, cifar100]
  Optimizer:
    weight-decay: [1.e-6]
    # lr: [1000, 300, 100, 30, 10, 5, 2, 1]
    # weight-decay: [5.e-4, 2.e-3, 1.e-4]
...
